{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"M1ogGKcIvJAi"},"outputs":[],"source":["import pandas as pd\n","import torch\n","import numpy as np\n","import os\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from tab_transformer_pytorch import TabTransformer\n","import logging\n","\n","\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",")\n","\n","def extract_tab_features(csv_path, feature_dim=256):\n","    if not os.path.exists(csv_path):\n","        raise FileNotFoundError(f\"path error: {csv_path}\")\n","    if not isinstance(feature_dim, int) or feature_dim <= 0:\n","        raise ValueError(\"feature_dim error\")\n","\n","\n","\n","    file_name = os.path.splitext(os.path.basename(csv_path))[0]\n","\n","\n","    try:\n","        df = pd.read_csv(csv_path)\n","        logging.info(f\"datashape: {df.shape}\")\n","\n","        cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n","        num_cols = df.select_dtypes(include=['number']).columns.tolist()\n","\n","\n","\n","\n","        encoders = {}\n","        encoded_df = df.copy()\n","        for col in cat_cols:\n","            le = LabelEncoder()\n","            encoded_df[col] = le.fit_transform(df[col].astype(str))\n","            encoders[col] = le\n","\n","        scaler = StandardScaler()\n","        if num_cols:\n","            if encoded_df[num_cols].isnull().values.any():\n","                encoded_df[num_cols] = encoded_df[num_cols].fillna(encoded_df[num_cols].mean())\n","            encoded_df[num_cols] = scaler.fit_transform(encoded_df[num_cols])\n","\n","\n","        X_cat = encoded_df[cat_cols].values if cat_cols else np.zeros((len(encoded_df), 0))\n","        X_num = encoded_df[num_cols].values if num_cols else np.zeros((len(encoded_df), 0))\n","\n","\n","        categories = [encoded_df[col].nunique() for col in cat_cols]\n","        if not categories:\n","            categories = [1]\n","            X_cat = np.zeros((len(encoded_df), 1), dtype=int)\n","\n","\n","        dim = max(feature_dim // 4, 8)\n","        model = TabTransformer(\n","            categories=categories,\n","            num_continuous=X_num.shape[1],\n","            dim=dim,\n","            depth=6,\n","            heads=8,\n","            attn_dropout=0.1,\n","            ff_dropout=0.1,\n","            mlp_hidden_mults=(4, 2),\n","            mlp_act=None,\n","            dim_out=feature_dim\n","        )\n","\n","\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        model = model.to(device)\n","        logging.info(f\"model loaded in : {device}\")\n","\n","\n","        X_cat_tensor = torch.LongTensor(X_cat).to(device)\n","        X_num_tensor = torch.FloatTensor(X_num).to(device)\n","\n","\n","        model.eval()\n","        batch_size = 64\n","        num_samples = len(encoded_df)\n","        all_features = []\n","\n","        with torch.no_grad():\n","            for i in range(0, num_samples, batch_size):\n","                end_idx = min(i + batch_size, num_samples)\n","                cat_batch = X_cat_tensor[i:end_idx]\n","                num_batch = X_num_tensor[i:end_idx]\n","\n","\n","                features = model(cat_batch, num_batch).cpu()\n","                all_features.append(features.numpy())\n","\n","                if (i // batch_size) % 10 == 0:\n","                    logging.info(f\"process: {end_idx}/{num_samples}\")\n","\n","\n","        all_features = np.vstack(all_features)\n","\n","        logging.info(f\"features.shape: {all_features.shape}\")\n","\n","\n","        print(\"features：\")\n","        print(all_features)\n","\n","        return all_features\n","\n","    except Exception as e:\n","        logging.error(f\"error: {str(e)}\")\n","        import traceback\n","        traceback.print_exc()\n","        return None\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nUa2PQ80yOIz","executionInfo":{"status":"ok","timestamp":1744004789328,"user_tz":-600,"elapsed":17687,"user":{"displayName":"Andy Ai","userId":"07063483985315465265"}},"outputId":"877e3831-f2a3-458f-bac2-4b13b13a4f78"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["csv_path_mutation = \"/content/drive/MyDrive/proj72_data/multiomics/mutation.csv\"\n","csv_path_clinical = \"/content/drive/MyDrive/proj72_data/multiomics/clinical.csv\"\n","feature_dim = 256\n","\n","\n","features_mutation = extract_tab_features(\n","    csv_path=csv_path_mutation,\n","    feature_dim=feature_dim\n",")\n","\n","features_clinical = extract_tab_features(\n","    csv_path=csv_path_clinical,\n","    feature_dim=feature_dim\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DtZjcHBax_6e","executionInfo":{"status":"ok","timestamp":1744005847092,"user_tz":-600,"elapsed":177753,"user":{"displayName":"Andy Ai","userId":"07063483985315465265"}},"outputId":"9d91fba1-5d78-4020-b26a-76d7bc17b226"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["features：\n","[[ 0.11898579  0.18346208  0.058261   ... -0.02295681 -0.19146286\n","   0.05482033]\n"," [ 0.1495759   0.14784636  0.1287416  ...  0.05690724 -0.0748592\n","  -0.19670159]\n"," [ 0.22605298  0.14434478  0.14035615 ... -0.09968987 -0.24286707\n","  -0.05302662]\n"," ...\n"," [-0.04441394  0.12801169  0.04246295 ...  0.01180192 -0.14561115\n","  -0.11234135]\n"," [-0.01789065  0.17041911  0.00188292 ... -0.02076292 -0.14894941\n","  -0.04929658]\n"," [ 0.0753727   0.23229182  0.05958675 ... -0.0898457  -0.1872366\n","  -0.12321194]]\n","features：\n","[[ 0.30354282  0.5335673   0.77190095 ... -0.31558722 -0.35376155\n","   0.17969918]\n"," [ 0.10730834  0.28623575  0.2597753  ... -0.12229361 -0.15587553\n","   0.17283902]\n"," [ 0.10702241  0.12587878  0.52760005 ... -0.25977504 -0.5123722\n","   0.16054098]\n"," ...\n"," [-0.44602314  0.42073148  0.02070296 ... -0.34694073  0.20849368\n","   0.05358921]\n"," [ 0.05102543  0.39582673  0.20700967 ... -0.558087   -0.04459652\n","   0.20244083]\n"," [ 0.07341093  0.49012336  0.05591625 ... -0.6342342  -0.31341708\n","   0.10049491]]\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from typing import List\n","from typing import Dict\n","import torch.nn.functional as F\n","import pandas as pd\n","import numpy as np\n","import itertools"],"metadata":{"id":"zZhute71jNnO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","class FowardNetwork(nn.Module):\n","    def __init__(self, embed_dim):\n","        super(FowardNetwork, self).__init__()\n","        self.Fc1 = nn.Linear(embed_dim, embed_dim, bias=True)\n","        self.Fc2 = nn.Linear(embed_dim, embed_dim, bias=True)\n","\n","    def forward(self, x):\n","        x = F.silu(self.Fc1(x))\n","        x = F.silu(self.Fc2(x))\n","        return x\n","\n","\n","class CrossAttention(nn.Module):\n","    def __init__(self, embed_dim, num_heads, batch_size):\n","        super(CrossAttention, self).__init__()\n","        self.dropout = 0.2\n","        self.batch_size = batch_size\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.head_dim = self.embed_dim // self.num_heads\n","        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)\n","        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)\n","        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)\n","        self.O_layer = nn.Linear(embed_dim, embed_dim, bias=False)\n","        self.norm1 = nn.LayerNorm(embed_dim)\n","        self.norm2 = nn.LayerNorm(embed_dim)\n","        self.drop1 = nn.Dropout(self.dropout)\n","        self.drop2 = nn.Dropout(self.dropout)\n","        self.alpha = nn.Parameter(torch.tensor(0.2))\n","        self.belta = nn.Parameter(torch.ones(num_heads))\n","        self.fowNet = FowardNetwork(self.embed_dim)\n","\n","    def split_heads(self, x):\n","        x = x.view(self.batch_size, -1, self.num_heads, self.head_dim)\n","        return x.permute(0, 2, 1, 3)\n","\n","    def scaled_dot_product_attention(self, Q, K, V):\n","        scores = (torch.matmul(Q, K.transpose(-1, -2)) /\n","                  torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float64)))\n","        attn_weights = F.softmax(scores, dim=-1)\n","        original_mask = torch.zeros_like(attn_weights)\n","        mask_indices = (attn_weights >= self.alpha).float()\n","        natural_index = torch.arange(0, attn_weights.size(3))\n","        natural_index = natural_index[None,None,None,:].expand(self.batch_size,\n","                                                self.num_heads,attn_weights.size(2), -1)\n","        original_mask.scatter_(-1, natural_index, src=mask_indices)\n","        attn_weights = attn_weights * original_mask\n","        attn_weights_adjusted = F.softmax(attn_weights, dim=-1)\n","\n","        attn_output = torch.matmul(attn_weights_adjusted, V)\n","        return attn_output, attn_weights\n","\n","    def forward(self, query, key, value):\n","\n","        Q = self.split_heads(self.W_q(query))\n","        K = self.split_heads(self.W_k(key))\n","        V = self.split_heads(self.W_v(value))\n","        attn_output, atten_maps = self.scaled_dot_product_attention(Q, K, V)\n","        attn_output = attn_output.transpose(1, 2).contiguous()\n","        attn_output = attn_output.view(self.batch_size, query.size(1), self.embed_dim)\n","        attn_output = self.O_layer(attn_output)\n","        attn_output = self.norm1(query + self.drop1(attn_output))\n","        inter_output = self.fowNet(attn_output)\n","        final_output = self.norm2(attn_output + self.drop2(inter_output))\n","\n","        return final_output, atten_maps"],"metadata":{"id":"tkp5l9Ld0IkC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DynamicSequentialMultiheadCrossAttention(nn.Module):\n","    def __init__(self, d_model: int, total_modalities: int, embed_dim: int, num_heads: int, batch_size: int):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.total_modalities = total_modalities  # Total number of modality\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.batch_size = batch_size\n","\n","        # Initial multihead cross attention models\n","        self.cross_attentions = nn.ModuleList([\n","            CrossAttention(embed_dim=self.embed_dim, num_heads=self.num_heads, batch_size=self.batch_size)\n","            for _ in range(self.total_modalities - 1)\n","        ])\n","\n","    def forward(self, *modalities: List[torch.Tensor]):\n","        \"\"\"\n","        Args:\n","            modalities: List of tensors where:\n","                - modalities[0]: Main modality A [B, N, d_model] (Query)\n","                - modalities[1:]: Auxiliary modalities [B, M_i, d_model] (Keys/Values)\n","        Returns:\n","            a_enhanced: [B, N, d_model] (Enhanced main modality)\n","        \"\"\"\n","        assert len(modalities) == self.total_modalities, \\\n","            f\"Expected {self.total_modalities} modalities (including A), got {len(modalities)}\"\n","\n","        a = modalities[0]  # Main modality A\n","\n","        # Sequentially fuse each auxiliary modality\n","        for i in range(1, self.total_modalities):\n","            # Current auxiliary modality (B, C, etc.)\n","            current_modality = modalities[i]\n","            # Cross-Attention: A as Query, current modality as Key/Value\n","            a, attn_weights = self.cross_attentions[i - 1](a, current_modality, current_modality)\n","\n","        return a, attn_weights"],"metadata":{"id":"u8bLEvesvOcH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#batch_size = 64\n","#feature_dim = 256\n","#seq_length_c = features_clinical.shape[0] // batch_size\n","#seq_length_mut = features_mutation.shape[0] // batch_size\n","\n","#features_mutation = features_mutation.reshape(batch_size, seq_length_mut, feature_dim)\n","#features_clinical = features_clinical.reshape(batch_size, seq_length_c, feature_dim)\n"],"metadata":{"id":"QDQ9C_Igx2GF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def reshape_with_padding(features, batch_size=64, feature_dim=256):\n","    total_samples = features.shape[0]\n","    seq_length = int(np.ceil(total_samples / batch_size))\n","    target_total = batch_size * seq_length\n","    padding_needed = target_total - total_samples\n","\n","    if padding_needed > 0:\n","        pad = np.zeros((padding_needed, feature_dim))\n","        features = np.vstack([features, pad])\n","\n","    features = features.reshape(batch_size, seq_length, feature_dim)\n","    return features\n"],"metadata":{"id":"ElUSL0RX1ayu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 64\n","feature_dim = 256\n","\n","features_mutation = reshape_with_padding(features_mutation, batch_size, feature_dim)\n","features_clinical = reshape_with_padding(features_clinical, batch_size, feature_dim)\n"],"metadata":{"id":"wCrVcdQl3qUK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","features_clinical = torch.tensor(features_clinical, dtype=torch.float32).to(device)\n","features_mutation = torch.tensor(features_mutation, dtype=torch.float32).to(device)\n","\n","model = DynamicSequentialMultiheadCrossAttention(d_model=256, total_modalities=2, embed_dim=256, num_heads=8, batch_size=64).to(device)\n","\n","output, attn_weights = model(features_clinical, features_mutation)\n","print(output.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZwV3Z9ZH4Arx","executionInfo":{"status":"ok","timestamp":1744006288705,"user_tz":-600,"elapsed":83,"user":{"displayName":"Andy Ai","userId":"07063483985315465265"}},"outputId":"4952c284-276a-4193-bb6a-ee76efdfcd0e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([64, 21, 256])\n"]}]},{"cell_type":"code","source":["model = DynamicSequentialMultiheadCrossAttention(d_model=256, total_modalities=2, embed_dim=256, num_heads=8, batch_size=64)\n","output, attn_weights = model(features_clinical, features_mutation)\n","print(output.shape)"],"metadata":{"id":"AerhtofQzpwx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744006291389,"user_tz":-600,"elapsed":34,"user":{"displayName":"Andy Ai","userId":"07063483985315465265"}},"outputId":"53220d21-3739-4099-8b5a-5ea43dcdac93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([64, 21, 256])\n"]}]},{"cell_type":"code","source":["output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_g2r6A2j4EzP","executionInfo":{"status":"ok","timestamp":1744006308889,"user_tz":-600,"elapsed":233,"user":{"displayName":"Andy Ai","userId":"07063483985315465265"}},"outputId":"c3645d6e-a183-4fad-def4-03056f1a1b7b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 0.9468,  1.3098,  2.2585,  ..., -0.7706, -0.7170,  0.8301],\n","         [ 0.4157,  0.4868,  0.6769,  ..., -0.2153, -0.3273,  0.5964],\n","         [ 0.3138, -0.1144,  1.2133,  ..., -0.5826, -1.0286,  0.3885],\n","         ...,\n","         [ 0.1701, -0.7020,  1.0758,  ...,  0.3533,  0.6397,  0.7606],\n","         [ 0.3512,  0.1266,  1.2712,  ..., -0.5711,  0.4935,  0.8700],\n","         [-0.1589,  1.0831,  1.0249,  ..., -1.7126, -1.8269,  0.7927]],\n","\n","        [[ 0.6620, -0.0300,  0.5703,  ..., -0.1709, -0.9496,  0.6046],\n","         [ 0.3956,  0.5757, -0.2135,  ..., -1.4176, -0.2242,  0.2286],\n","         [ 0.6683,  0.1578,  0.7205,  ..., -1.2043, -0.8021,  0.5351],\n","         ...,\n","         [ 0.3216,  0.9024,  0.6244,  ..., -0.5191,  0.3778,  0.4298],\n","         [-1.1257,  0.4531, -0.5790,  ..., -0.4395,  0.0533,  1.4077],\n","         [-0.7021,  1.6748,  1.1322,  ..., -0.7760, -1.0045,  1.0170]],\n","\n","        [[-0.4882,  1.4057,  0.6133,  ..., -0.4880,  0.4032,  0.9318],\n","         [ 0.3269,  1.1047,  1.3261,  ..., -0.2465,  0.5727,  0.9728],\n","         [ 0.2663,  0.7919, -0.7820,  ..., -1.2970, -0.7777,  1.1554],\n","         ...,\n","         [ 1.0260,  0.5622,  0.7391,  ..., -0.9237, -0.2354,  0.1661],\n","         [ 0.6847,  0.9453,  0.6878,  ..., -0.6106, -0.2476,  1.4987],\n","         [ 0.8007,  0.4770,  0.7498,  ..., -1.1689,  0.0264,  0.1692]],\n","\n","        ...,\n","\n","        [[ 0.6299,  0.1318,  0.7506,  ..., -0.9458,  0.6946,  1.4761],\n","         [-0.7012,  0.6460,  1.2736,  ..., -1.7446, -1.2620,  0.2318],\n","         [ 0.2310, -0.9485,  0.8081,  ..., -0.2943,  0.3262,  1.5594],\n","         ...,\n","         [-0.0768,  1.3099, -0.0343,  ..., -1.4804, -0.3290,  0.6082],\n","         [-0.5812,  0.0350,  0.3035,  ..., -1.2588,  0.1572,  1.7317],\n","         [ 1.2270,  0.1133,  0.6968,  ..., -1.4206,  0.1822,  0.1703]],\n","\n","        [[-0.0614,  0.3930,  1.6124,  ..., -0.2686, -0.5163,  1.0148],\n","         [ 1.0736,  1.3210,  0.8552,  ..., -0.8707, -0.9962,  0.9028],\n","         [ 0.7151,  0.1594,  1.1542,  ..., -0.2988, -1.5059,  0.1855],\n","         ...,\n","         [ 0.4098,  0.8110,  1.7680,  ...,  0.4069, -0.7227,  1.2445],\n","         [-0.1379,  1.6028,  2.2154,  ..., -0.9204, -0.1254,  0.7023],\n","         [ 1.0849,  1.2065,  1.6341,  ..., -0.5686, -0.0945,  0.8278]],\n","\n","        [[-1.4309,  1.3818,  0.1393,  ..., -0.9585,  0.8748,  0.2118],\n","         [ 0.2676,  0.8569,  0.6411,  ..., -1.2405,  0.1502,  0.5078],\n","         [ 0.1609,  1.2108,  0.1186,  ..., -1.7000, -0.6505,  0.2171],\n","         ...,\n","         [-0.3751, -0.3421,  0.9752,  ...,  0.9016,  0.2796,  1.2390],\n","         [-0.3535, -0.3203, -0.0430,  ...,  0.9310,  0.3053,  1.2705],\n","         [-0.3427, -0.0286,  1.0311,  ...,  0.9562,  0.3235, -0.0286]]],\n","       grad_fn=<NativeLayerNormBackward0>)"]},"metadata":{},"execution_count":23}]}]}