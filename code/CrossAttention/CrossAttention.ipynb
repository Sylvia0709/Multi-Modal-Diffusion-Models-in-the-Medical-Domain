{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Cvjn6X6jH73"
   },
   "source": [
    "# Packages Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZhute71jNnO"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "from typing import Dict\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbBOpsecg6SY"
   },
   "source": [
    "# Data fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoF9CBJFhIRT"
   },
   "source": [
    "## 1. Cross Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1R_ixWykhTLh"
   },
   "source": [
    "### 1. Dynamic Sequential Cross Attention\n",
    "\n",
    "\n",
    "This is a single-head sequential cross attention function. The number of input modalities is dynamic. The shape of input modalities[ ] must be (batch_size, sequence_length, feature_dim). The main modality always serves as the Query, iteratively attending to other modalities as Key-Value pairs, with residual connections and layer normalization to preserve original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xAnUl9o0i3Sr"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDynamicSequentialCrossAttention\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, d_model: \u001b[38;5;28mint\u001b[39m, total_modalities: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class DynamicSequentialCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, total_modalities: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.total_modalities = total_modalities  # Total number of modality\n",
    "\n",
    "        # Number of Cross-Attention layers = total_modalities - 1 (A→B, A→C, etc.)\n",
    "        self.cross_attentions = nn.ModuleList([\n",
    "            nn.MultiheadAttention(embed_dim=d_model, num_heads=1, batch_first=True)\n",
    "            for _ in range(total_modalities - 1)\n",
    "        ])\n",
    "\n",
    "        # Normalization layers\n",
    "        self.norms = nn.ModuleList([\n",
    "            nn.LayerNorm(d_model)\n",
    "            for _ in range(total_modalities - 1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, *modalities: List[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            modalities: List of tensors where:\n",
    "                - modalities[0]: Main modality A [B, N, d_model] (Query)\n",
    "                - modalities[1:]: Auxiliary modalities [B, M_i, d_model] (Keys/Values)\n",
    "        Returns:\n",
    "            a_enhanced: [B, N, d_model] (Enhanced main modality)\n",
    "        \"\"\"\n",
    "        assert len(modalities) == self.total_modalities, \\\n",
    "            f\"Expected {self.total_modalities} modalities (including A), got {len(modalities)}\"\n",
    "\n",
    "        a = modalities[0]  # Main modality A\n",
    "\n",
    "        # Sequentially fuse each auxiliary modality\n",
    "        for i in range(1, self.total_modalities):\n",
    "            # Current auxiliary modality (B, C, etc.)\n",
    "            current_modality = modalities[i]\n",
    "\n",
    "            # Cross-Attention: A as Query, current modality as Key/Value\n",
    "            attn_output, _ = self.cross_attentions[i-1](\n",
    "                query=a,\n",
    "                key=current_modality,\n",
    "                value=current_modality\n",
    "            )\n",
    "\n",
    "            # # Residual connection + LayerNorm\n",
    "            # a = a + attn_output\n",
    "            # a = self.norms[i-1](a)\n",
    "\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8EERbWfjSuI"
   },
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GLB7v-WYi54V",
    "outputId": "6863f4f2-6020-4cdf-9495-0e7908e01cbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "model = DynamicSequentialCrossAttention(d_model=256, total_modalities=3) # a, b, c, 3 modalities align to a\n",
    "\n",
    "# Inputs (order matters: A first, then B, C)\n",
    "a = torch.randn(32, 10, 256)  # Main modality (Query), it should be clinical data\n",
    "b = torch.randn(32, 20, 256)  # Auxiliary modality 1\n",
    "c = torch.randn(32, 15, 256)  # Auxiliary modality 2\n",
    "\n",
    "output = model(a, b, c)  # A enhanced by B and C sequentially\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9x_rxRzWQnog"
   },
   "source": [
    "### 2. Multihead Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_hZDFp215XOp"
   },
   "outputs": [],
   "source": [
    "\n",
    "# device = torch.device(\"cuda\")\n",
    "# torch.cuda.set_device(0)\n",
    "# torch.cuda.synchronize()\n",
    "class FowardNetwork(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(FowardNetwork, self).__init__()\n",
    "        self.Fc1 = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.Fc2 = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.silu(self.Fc1(x))\n",
    "        x = F.silu(self.Fc2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, batch_size):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.dropout = 0.2\n",
    "        self.batch_size = batch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.O_layer = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop1 = nn.Dropout(self.dropout)\n",
    "        self.drop2 = nn.Dropout(self.dropout)\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.2))\n",
    "        self.belta = nn.Parameter(torch.ones(num_heads))\n",
    "        self.fowNet = FowardNetwork(self.embed_dim)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        x = x.view(self.batch_size, -1, self.num_heads, self.head_dim)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V):\n",
    "        scores = (torch.matmul(Q, K.transpose(-1, -2)) /\n",
    "                  torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float64)))\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        original_mask = torch.zeros_like(attn_weights)\n",
    "        mask_indices = (attn_weights >= self.alpha).float()\n",
    "        natural_index = torch.arange(0, attn_weights.size(3))\n",
    "        natural_index = natural_index[None,None,None,:].expand(self.batch_size,\n",
    "                                                self.num_heads,attn_weights.size(2), -1)\n",
    "        original_mask.scatter_(-1, natural_index, src=mask_indices)\n",
    "        attn_weights = attn_weights * original_mask\n",
    "        attn_weights_adjusted = F.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights_adjusted, V)\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "\n",
    "        Q = self.split_heads(self.W_q(query))\n",
    "        K = self.split_heads(self.W_k(key))\n",
    "        V = self.split_heads(self.W_v(value))\n",
    "        attn_output, atten_maps = self.scaled_dot_product_attention(Q, K, V)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(self.batch_size, query.size(1), self.embed_dim)\n",
    "        attn_output = self.O_layer(attn_output)\n",
    "        attn_output = self.norm1(query + self.drop1(attn_output))\n",
    "        inter_output = self.fowNet(attn_output)\n",
    "        final_output = self.norm2(attn_output + self.drop2(inter_output))\n",
    "\n",
    "        return final_output, atten_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Y80ZE3M5xNG"
   },
   "outputs": [],
   "source": [
    "#Example\n",
    "embed_dim=256\n",
    "num_heads=8\n",
    "batch_size=32\n",
    "model= CrossAttention(embed_dim, num_heads, batch_size)\n",
    "a = torch.randn(32, 12, 256)  # A as main\n",
    "b = torch.randn(32, 12, 256)   # B\n",
    "c = torch.randn(32, 12, 256)  # C\n",
    "integated_data, attn_weights = model.forward(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BF5-PK4Qr7L"
   },
   "source": [
    "### 3. Dynamic Sequential Multihead Cross Attention\n",
    "\n",
    "This is the combination of sequential and multihead cross attention.\n",
    "\n",
    "The difference with sequential is cross attention part in forward()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ie8Ag2vOf3_"
   },
   "outputs": [],
   "source": [
    "class DynamicSequentialMultiheadCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, total_modalities: int, embed_dim: int, num_heads: int, batch_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.total_modalities = total_modalities  # Total number of modality\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Initial multihead cross attention models\n",
    "        self.cross_attentions = nn.ModuleList([\n",
    "            CrossAttention(embed_dim=self.embed_dim, num_heads=self.num_heads, batch_size=self.batch_size)\n",
    "            for _ in range(self.total_modalities - 1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, *modalities: List[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            modalities: List of tensors where:\n",
    "                - modalities[0]: Main modality A [B, N, d_model] (Query)\n",
    "                - modalities[1:]: Auxiliary modalities [B, M_i, d_model] (Keys/Values)\n",
    "        Returns:\n",
    "            a_enhanced: [B, N, d_model] (Enhanced main modality)\n",
    "        \"\"\"\n",
    "        assert len(modalities) == self.total_modalities, \\\n",
    "            f\"Expected {self.total_modalities} modalities (including A), got {len(modalities)}\"\n",
    "\n",
    "        a = modalities[0]  # Main modality A\n",
    "\n",
    "        # Sequentially fuse each auxiliary modality\n",
    "        for i in range(1, self.total_modalities):\n",
    "            # Current auxiliary modality (B, C, etc.)\n",
    "            current_modality = modalities[i]\n",
    "            # Cross-Attention: A as Query, current modality as Key/Value\n",
    "            a, attn_weights = self.cross_attentions[i - 1](a, current_modality, current_modality)\n",
    "\n",
    "        return a, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVKMt64jUJPK"
   },
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJ0c7gdnUIZx",
    "outputId": "ed320a1e-a20b-4016-f2ec-78814469fe8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "model = DynamicSequentialMultiheadCrossAttention(d_model=256, total_modalities=3, embed_dim=256, num_heads=8, batch_size=32)\n",
    "\n",
    "a = torch.randn(32, 10, 256)  # A as main\n",
    "b = torch.randn(32, 8, 256)   # B\n",
    "c = torch.randn(32, 12, 256)  # C\n",
    "\n",
    "output, attn_weights = model(a, b, c)\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
